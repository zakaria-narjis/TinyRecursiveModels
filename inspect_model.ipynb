{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ae6352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL LOADED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "Model Configuration:\n",
      "  H_cycles: 3\n",
      "  H_layers: 0\n",
      "  L_cycles: 6\n",
      "  L_layers: 2\n",
      "  expansion: 4\n",
      "  forward_dtype: bfloat16\n",
      "  halt_exploration_prob: 0.1\n",
      "  halt_max_steps: 16\n",
      "  hidden_size: 512\n",
      "  mlp_t: True\n",
      "  no_ACT_continue: True\n",
      "  num_heads: 8\n",
      "  pos_encodings: none\n",
      "  puzzle_emb_len: 16\n",
      "  puzzle_emb_ndim: 512\n",
      "  causal: False\n",
      "  vocab_size: 11\n",
      "  seq_len: 81\n",
      "  num_puzzle_identifiers: 1\n",
      "  batch_size: 768\n",
      "\n",
      "Total parameters: 5,028,866\n",
      "Trainable parameters: 5,028,866\n",
      "\n",
      "================================================================================\n",
      "MODEL STRUCTURE (HIGH LEVEL)\n",
      "================================================================================\n",
      "ACTLossHead(\n",
      "  (model): TinyRecursiveReasoningModel_ACTV1(\n",
      "    (inner): TinyRecursiveReasoningModel_ACTV1_Inner(\n",
      "      (embed_tokens): CastedEmbedding()\n",
      "      (lm_head): CastedLinear()\n",
      "      (q_head): CastedLinear()\n",
      "      (puzzle_emb): CastedSparseEmbedding()\n",
      "      (L_level): TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TinyRecursiveReasoningModel_ACTV1Block(\n",
      "            (mlp_t): SwiGLU(\n",
      "              (gate_up_proj): CastedLinear()\n",
      "              (down_proj): CastedLinear()\n",
      "            )\n",
      "            (mlp): SwiGLU(\n",
      "              (gate_up_proj): CastedLinear()\n",
      "              (down_proj): CastedLinear()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "================================================================================\n",
      "INPUT/OUTPUT TENSOR SHAPES\n",
      "================================================================================\n",
      "\n",
      "Batch structure:\n",
      "  inputs: torch.Tensor of shape (1, 81)\n",
      "    - dtype: torch.int32 or torch.long\n",
      "    - values: token IDs in range [0, 10]\n",
      "\n",
      "  labels: torch.Tensor of shape (1, 81)\n",
      "    - dtype: torch.int32 or torch.long\n",
      "    - values: token IDs in range [0, 10] or -100 (ignore)\n",
      "\n",
      "  puzzle_identifiers: torch.Tensor of shape (1,)\n",
      "    - dtype: torch.int32 or torch.long\n",
      "    - values: puzzle IDs in range [0, 0]\n",
      "\n",
      "Output structure:\n",
      "  logits: torch.Tensor of shape (1, 81, 11)\n",
      "    - unnormalized logits for each token position\n",
      "\n",
      "================================================================================\n",
      "CREATING DUMMY BATCH FOR TESTING\n",
      "================================================================================\n",
      "\n",
      "Dummy batch created:\n",
      "  inputs: torch.Size([1, 81]), dtype=torch.int64, device=cuda:0\n",
      "  labels: torch.Size([1, 81]), dtype=torch.int64, device=cuda:0\n",
      "  puzzle_identifiers: torch.Size([1]), dtype=torch.int64, device=cuda:0\n",
      "\n",
      "Initializing carry...\n",
      "Carry initialized: <class 'models.recursive_reasoning.trm.TinyRecursiveReasoningModel_ACTV1Carry'>\n",
      "  inner_carry.z_H: torch.Size([1, 97, 512])\n",
      "  inner_carry.z_L: torch.Size([1, 97, 512])\n",
      "  steps: torch.Size([1])\n",
      "  halted: torch.Size([1])\n",
      "\n",
      "Running forward pass...\n",
      "\n",
      "Forward pass complete!\n",
      "  loss: 5.7451\n",
      "  all_finish: False\n",
      "  logits shape: torch.Size([1, 81, 11])\n",
      "  metrics: ['count', 'accuracy', 'exact_accuracy', 'q_halt_accuracy', 'steps', 'lm_loss', 'q_halt_loss']\n",
      "  predicted tokens shape: torch.Size([1, 81])\n",
      "\n",
      "================================================================================\n",
      "READY FOR INSPECTION!\n",
      "================================================================================\n",
      "\n",
      "Available objects:\n",
      "  - model: the full model (ACTLossHead)\n",
      "  - model.model: the ACT wrapper\n",
      "  - model.model.inner: the inner transformer model\n",
      "  - full_config: the full training config\n",
      "  - model_cfg: the model config\n",
      "  - dummy_batch: a sample batch\n",
      "  - carry: the model's recurrent state\n",
      "  - preds: predictions from forward pass\n",
      "\n",
      "Useful inspection functions:\n",
      "  - inspect_model_structure(model): detailed layer breakdown\n",
      "  - inspect_state_dict(model): all parameter shapes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from utils.functions import load_model_class\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "CHECKPOINT_PATH = \"/home/zakarianarjis/workspace/TinyRecursiveModels/checkpoints/Sudoku-extreme-1k-aug-1000-ACT-torch/pretrain_mlp_t_sudoku/step_65100\"\n",
    "DATA_PATH = \"data/sudoku-extreme-1k-aug-1000\"  # Need this to get dataset metadata\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def strip_compiled_prefix(state_dict):\n",
    "    \"\"\"Remove _orig_mod. prefix from compiled model state dict.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('_orig_mod.'):\n",
    "            new_key = key.replace('_orig_mod.', '')\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def get_dataset_metadata(data_path, split=\"test\"):\n",
    "    \"\"\"Load dataset metadata without loading the full dataset.\"\"\"\n",
    "    import json\n",
    "    from dataset.common import PuzzleDatasetMetadata\n",
    "    \n",
    "    metadata_file = os.path.join(data_path, split, \"dataset.json\")\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = PuzzleDatasetMetadata(**json.load(f))\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_model(checkpoint_path, data_path=None, device=\"cuda\"):\n",
    "    \"\"\"Load model from checkpoint.\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    config_file = os.path.join(os.path.dirname(checkpoint_path), \"all_config.yaml\")\n",
    "    with open(config_file, \"r\") as f:\n",
    "        full_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Extract model config\n",
    "    arch_config = full_config[\"arch\"]\n",
    "    model_name = arch_config[\"name\"]\n",
    "    loss_config = arch_config[\"loss\"]\n",
    "    loss_name = loss_config[\"name\"]\n",
    "    \n",
    "    # Build model config\n",
    "    model_cfg = {k: v for k, v in arch_config.items() if k not in [\"name\", \"loss\"]}\n",
    "    model_cfg[\"causal\"] = False\n",
    "    \n",
    "    # Get dataset metadata if data_path provided, otherwise use defaults from config\n",
    "    if data_path is not None:\n",
    "        metadata = get_dataset_metadata(data_path)\n",
    "        model_cfg[\"vocab_size\"] = metadata.vocab_size\n",
    "        model_cfg[\"seq_len\"] = metadata.seq_len\n",
    "        model_cfg[\"num_puzzle_identifiers\"] = metadata.num_puzzle_identifiers\n",
    "        model_cfg[\"batch_size\"] = full_config.get(\"global_batch_size\", 64)\n",
    "    else:\n",
    "        # Try to infer from training config\n",
    "        train_data_path = full_config[\"data_paths\"][0]\n",
    "        if os.path.exists(train_data_path):\n",
    "            metadata = get_dataset_metadata(train_data_path, split=\"train\")\n",
    "            model_cfg[\"vocab_size\"] = metadata.vocab_size\n",
    "            model_cfg[\"seq_len\"] = metadata.seq_len\n",
    "            model_cfg[\"num_puzzle_identifiers\"] = metadata.num_puzzle_identifiers\n",
    "            model_cfg[\"batch_size\"] = full_config.get(\"global_batch_size\", 64)\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot find dataset at {train_data_path}. Please provide data_path argument.\")\n",
    "    \n",
    "    # Load classes\n",
    "    model_cls = load_model_class(model_name)\n",
    "    loss_head_cls = load_model_class(loss_name)\n",
    "    loss_kwargs = {k: v for k, v in loss_config.items() if k != \"name\"}\n",
    "    \n",
    "    # Create model\n",
    "    with torch.device(device):\n",
    "        model = model_cls(model_cfg)\n",
    "        model = loss_head_cls(model, **loss_kwargs)\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        state_dict = strip_compiled_prefix(state_dict)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.eval()\n",
    "    \n",
    "    return model, full_config, model_cfg\n",
    "\n",
    "def inspect_model_structure(model):\n",
    "    \"\"\"Print detailed model structure.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED MODEL STRUCTURE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nLayer breakdown:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only leaf modules\n",
    "            num_params = sum(p.numel() for p in module.parameters())\n",
    "            if num_params > 0:\n",
    "                print(f\"  {name}: {module.__class__.__name__} ({num_params:,} params)\")\n",
    "\n",
    "def inspect_state_dict(model):\n",
    "    \"\"\"Print state dict keys and shapes.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL STATE DICT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, param in model.state_dict().items():\n",
    "        print(f\"  {name}: {tuple(param.shape)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Model\n",
    "# ============================================================================\n",
    "\n",
    "model, full_config, model_cfg = load_model(CHECKPOINT_PATH, DATA_PATH)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print config\n",
    "print(\"\\nModel Configuration:\")\n",
    "for key, value in model_cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL STRUCTURE (HIGH LEVEL)\")\n",
    "print(\"=\" * 80)\n",
    "print(model)\n",
    "\n",
    "# ============================================================================\n",
    "# Input/Output Shapes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INPUT/OUTPUT TENSOR SHAPES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = model_cfg[\"seq_len\"]\n",
    "vocab_size = model_cfg[\"vocab_size\"]\n",
    "num_puzzle_identifiers = model_cfg[\"num_puzzle_identifiers\"]\n",
    "\n",
    "print(f\"\\nBatch structure:\")\n",
    "print(f\"  inputs: torch.Tensor of shape ({batch_size}, {seq_len})\")\n",
    "print(f\"    - dtype: torch.int32 or torch.long\")\n",
    "print(f\"    - values: token IDs in range [0, {vocab_size - 1}]\")\n",
    "print(f\"\\n  labels: torch.Tensor of shape ({batch_size}, {seq_len})\")\n",
    "print(f\"    - dtype: torch.int32 or torch.long\")\n",
    "print(f\"    - values: token IDs in range [0, {vocab_size - 1}] or -100 (ignore)\")\n",
    "print(f\"\\n  puzzle_identifiers: torch.Tensor of shape ({batch_size},)\")\n",
    "print(f\"    - dtype: torch.int32 or torch.long\")\n",
    "print(f\"    - values: puzzle IDs in range [0, {num_puzzle_identifiers - 1}]\")\n",
    "\n",
    "print(f\"\\nOutput structure:\")\n",
    "print(f\"  logits: torch.Tensor of shape ({batch_size}, {seq_len}, {vocab_size})\")\n",
    "print(f\"    - unnormalized logits for each token position\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create a dummy batch for testing\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING DUMMY BATCH FOR TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dummy_batch = {\n",
    "    \"inputs\": torch.randint(0, vocab_size, (batch_size, seq_len), device=\"cuda\"),\n",
    "    \"labels\": torch.randint(0, vocab_size, (batch_size, seq_len), device=\"cuda\"),\n",
    "    \"puzzle_identifiers\": torch.randint(0, num_puzzle_identifiers, (batch_size,), device=\"cuda\"),\n",
    "}\n",
    "\n",
    "print(f\"\\nDummy batch created:\")\n",
    "for key, value in dummy_batch.items():\n",
    "    print(f\"  {key}: {value.shape}, dtype={value.dtype}, device={value.device}\")\n",
    "\n",
    "# Initialize carry\n",
    "print(\"\\nInitializing carry...\")\n",
    "with torch.device(\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        carry = model.initial_carry(dummy_batch)\n",
    "        print(f\"Carry initialized: {type(carry)}\")\n",
    "        print(f\"  inner_carry.z_H: {carry.inner_carry.z_H.shape}\")\n",
    "        print(f\"  inner_carry.z_L: {carry.inner_carry.z_L.shape}\")\n",
    "        print(f\"  steps: {carry.steps.shape}\")\n",
    "        print(f\"  halted: {carry.halted.shape}\")\n",
    "\n",
    "# Run one forward pass\n",
    "print(\"\\nRunning forward pass...\")\n",
    "with torch.no_grad():\n",
    "    carry, loss, metrics, preds, all_finish = model(\n",
    "        carry=carry,\n",
    "        batch=dummy_batch,\n",
    "        return_keys=[\"logits\"]\n",
    "    )\n",
    "    \n",
    "print(f\"\\nForward pass complete!\")\n",
    "print(f\"  loss: {loss.item():.4f}\")\n",
    "print(f\"  all_finish: {all_finish}\")\n",
    "print(f\"  logits shape: {preds['logits'].shape}\")\n",
    "print(f\"  metrics: {list(metrics.keys())}\")\n",
    "\n",
    "# Get predictions\n",
    "pred_tokens = torch.argmax(preds['logits'], dim=-1)\n",
    "print(f\"  predicted tokens shape: {pred_tokens.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"READY FOR INSPECTION!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAvailable objects:\")\n",
    "print(\"  - model: the full model (ACTLossHead)\")\n",
    "print(\"  - model.model: the ACT wrapper\")\n",
    "print(\"  - model.model.inner: the inner transformer model\")\n",
    "print(\"  - full_config: the full training config\")\n",
    "print(\"  - model_cfg: the model config\")\n",
    "print(\"  - dummy_batch: a sample batch\")\n",
    "print(\"  - carry: the model's recurrent state\")\n",
    "print(\"  - preds: predictions from forward pass\")\n",
    "print(\"\\nUseful inspection functions:\")\n",
    "print(\"  - inspect_model_structure(model): detailed layer breakdown\")\n",
    "print(\"  - inspect_state_dict(model): all parameter shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32da5af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACTLossHead(\n",
       "  (model): TinyRecursiveReasoningModel_ACTV1(\n",
       "    (inner): TinyRecursiveReasoningModel_ACTV1_Inner(\n",
       "      (embed_tokens): CastedEmbedding()\n",
       "      (lm_head): CastedLinear()\n",
       "      (q_head): CastedLinear()\n",
       "      (puzzle_emb): CastedSparseEmbedding()\n",
       "      (L_level): TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TinyRecursiveReasoningModel_ACTV1Block(\n",
       "            (mlp_t): SwiGLU(\n",
       "              (gate_up_proj): CastedLinear()\n",
       "              (down_proj): CastedLinear()\n",
       "            )\n",
       "            (mlp): SwiGLU(\n",
       "              (gate_up_proj): CastedLinear()\n",
       "              (down_proj): CastedLinear()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3eaaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test L_level with dummy input\n",
    "import torch\n",
    "\n",
    "L_level = model.model.inner.L_level\n",
    "hidden_size = model_cfg[\"hidden_size\"]\n",
    "seq_len_with_emb = model_cfg[\"seq_len\"] + model.model.inner.puzzle_emb_len\n",
    "batch_size_test = 1\n",
    "\n",
    "hidden_states = 10*torch.randn(batch_size_test, seq_len_with_emb, hidden_size, device=\"cuda\", dtype=torch.bfloat16)\n",
    "input_injection = torch.zeros(batch_size_test, seq_len_with_emb, hidden_size, device=\"cuda\", dtype=torch.bfloat16)\n",
    "cos_sin = model.model.inner.rotary_emb() if hasattr(model.model.inner, 'rotary_emb') else None\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = L_level(hidden_states=hidden_states, input_injection=input_injection, cos_sin=cos_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 1: ACT PONDER TIME\n",
      "================================================================================\n",
      "Available metrics keys: dict_keys(['count', 'accuracy', 'exact_accuracy', 'q_halt_accuracy', 'steps', 'lm_loss', 'q_halt_loss'])\n",
      "\n",
      "Could not find key 'act_steps' in metrics. Skipping ACT visualization.\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS 2: INPUT GRADIENT SALIENCY\n",
      "================================================================================\n",
      "\n",
      "Calculating saliency for prediction at position 40...\n",
      "  > Explaining prediction for token 4 at pos 40.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    129\u001b[39m SAMPLE_TO_EXPLAIN = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# First item in batch\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# NOTE: This uses 'dummy_batch'. For real insights, replace 'dummy_batch'\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# with a real data batch from your dataloader!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m saliency_map = \u001b[43mget_input_saliency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOS_TO_EXPLAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_TO_EXPLAIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# 6. Plot the saliency map\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m saliency_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mget_input_saliency\u001b[39m\u001b[34m(model, batch, position_to_explain, sample_index)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  > Explaining prediction for token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_token_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at pos \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition_to_explain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# 4. Backward pass: Calculate gradients\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# This will now work because `score_to_explain` has a grad_fn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mscore_to_explain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# 5. Get the saliency map from the gradient\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embeddings_tensor.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/trm/lib/python3.11/site-packages/torch/_tensor.py:629\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    621\u001b[39m         Tensor.backward,\n\u001b[32m    622\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    627\u001b[39m         inputs=inputs,\n\u001b[32m    628\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/trm/lib/python3.11/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/trm/lib/python3.11/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# We will use the 'model', 'dummy_batch', 'metrics', and 'preds'\n",
    "# variables already in your notebook's memory from the previous cell.\n",
    "\n",
    "# ============================================================================\n",
    "# Analysis 1: Visualize ACT Ponder Time\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS 1: ACT PONDER TIME\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# The 'metrics' dictionary from the forward pass should contain the \n",
    "# ponder time or step count for each token. We need to find the key.\n",
    "print(f\"Available metrics keys: {metrics.keys()}\")\n",
    "\n",
    "# !!! IMPORTANT !!!\n",
    "# Replace 'act_steps' with the actual key from the list above.\n",
    "# It might be 'ponder_time', 'steps', 'act_steps', or similar.\n",
    "# For this demo, we'll assume the key is 'act_steps'.\n",
    "PONDER_METRIC_KEY = 'act_steps' # <-- !!! CHECK AND CHANGE THIS KEY !!!\n",
    "\n",
    "if PONDER_METRIC_KEY in metrics:\n",
    "    # Fetch the ponder times [batch_size, seq_len]\n",
    "    ponder_times = metrics[PONDER_METRIC_KEY].cpu().detach().numpy()\n",
    "    \n",
    "    # Select the first puzzle in the batch to visualize\n",
    "    ponder_times_sample = ponder_times[0:1, :] # Keep 2D shape [1, seq_len]\n",
    "\n",
    "    print(f\"\\nVisualizing ponder time for sample 0 (Shape: {ponder_times_sample.shape})\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.imshow(ponder_times_sample, aspect='auto', cmap='viridis', interpolation='none')\n",
    "    plt.colorbar(label='Ponder Steps')\n",
    "    plt.xlabel('Token Position (Sequence Length)')\n",
    "    plt.yticks([])\n",
    "    plt.title('ACT Ponder Time (Computational Effort) per Token')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(\"> High values (brighter colors) show where the model 'thought' longer.\")\n",
    "    print(\"> This often corresponds to more difficult parts of the puzzle.\")\n",
    "    print(\"> NOTE: Since this is a 'dummy_batch' of random data, the pattern\")\n",
    "    print(\"  is meaningless. Run this with a *real* puzzle batch to see real insights.\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nCould not find key '{PONDER_METRIC_KEY}' in metrics. Skipping ACT visualization.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Analysis 2: Input Gradient Saliency (Final Correction)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS 2: INPUT GRADIENT SALIENCY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Saliency Helper Function (Corrected) ---\n",
    "def get_input_saliency(model, batch, position_to_explain, sample_index=0):\n",
    "    \"\"\"\n",
    "    Computes the gradient of a specific output logit w.r.t. input embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCalculating saliency for prediction at position {position_to_explain}...\")\n",
    "    \n",
    "    # We will store the original 'requires_grad' state for all parameters\n",
    "    original_grad_states = {}\n",
    "    \n",
    "    try:\n",
    "        # 0. Set model to eval mode BUT enable grads on all parameters\n",
    "        model.eval() \n",
    "        for name, p in model.named_parameters():\n",
    "            original_grad_states[name] = p.requires_grad\n",
    "            p.requires_grad = True # <<< THE KEY FIX\n",
    "            \n",
    "        model.zero_grad()\n",
    "\n",
    "        # 1. We need to \"hook\" the input embeddings\n",
    "        embeddings_tensor = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal embeddings_tensor\n",
    "            embeddings_tensor = output\n",
    "            embeddings_tensor.retain_grad() # Crucial to save grad\n",
    "        \n",
    "        hook = model.model.inner.embed_tokens.register_forward_hook(hook_fn)\n",
    "\n",
    "        # 2. Re-initialize carry and run forward pass *with grads enabled*\n",
    "        try:\n",
    "            with torch.device(\"cuda\"), torch.enable_grad():\n",
    "                carry = model.initial_carry(batch)\n",
    "                \n",
    "                # Forward pass *with* grads\n",
    "                carry, loss, metrics, preds, all_finish = model(\n",
    "                    carry=carry,\n",
    "                    batch=batch,\n",
    "                    return_keys=[\"logits\"]\n",
    "                )\n",
    "        finally:\n",
    "            hook.remove() # Always remove the hook!\n",
    "\n",
    "        if embeddings_tensor is None:\n",
    "            print(\"Error: Hook did not capture embeddings.\")\n",
    "            return\n",
    "\n",
    "        # 3. Define the \"decision\" to explain\n",
    "        logits_at_pos = preds['logits'][sample_index, position_to_explain] # [Vocab]\n",
    "        predicted_token_idx = torch.argmax(logits_at_pos).item()\n",
    "        score_to_explain = logits_at_pos[predicted_token_idx]\n",
    "\n",
    "        print(f\"  > Explaining prediction for token {predicted_token_idx} at pos {position_to_explain}.\")\n",
    "\n",
    "        # 4. Backward pass: Calculate gradients\n",
    "        # This should FINALLY work!\n",
    "        score_to_explain.backward()\n",
    "\n",
    "        # 5. Get the saliency map from the gradient\n",
    "        if embeddings_tensor.grad is None:\n",
    "            print(\"Error: Gradients were not populated on the embeddings tensor.\")\n",
    "            return\n",
    "            \n",
    "        saliency = embeddings_tensor.grad[sample_index].norm(dim=1) # [SeqLen]\n",
    "        \n",
    "        return saliency.cpu().detach().numpy()\n",
    "\n",
    "    finally:\n",
    "        # 6. Restore original parameter grad states\n",
    "        print(\"Restoring original model parameter grad states...\")\n",
    "        for name, p in model.named_parameters():\n",
    "            if name in original_grad_states:\n",
    "                p.requires_grad = original_grad_states[name]\n",
    "        print(\"Done.\")\n",
    "\n",
    "# --- Run Saliency ---\n",
    "\n",
    "# Explain the prediction at token position 40 (e.g., a specific Sudoku cell)\n",
    "POS_TO_EXPLAIN = 40 \n",
    "SAMPLE_TO_EXPLAIN = 0 # First item in batch\n",
    "\n",
    "# NOTE: This uses 'dummy_batch'. For real insights, replace 'dummy_batch'\n",
    "# with a real data batch from your dataloader!\n",
    "saliency_map = get_input_saliency(model, dummy_batch, POS_TO_EXPLAIN, SAMPLE_TO_EXPLAIN)\n",
    "\n",
    "# 7. Plot the saliency map\n",
    "if saliency_map is not None:\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.imshow(saliency_map[None, :], aspect='auto', cmap='hot', interpolation='none')\n",
    "    plt.colorbar(label='Saliency (Gradient Norm)')\n",
    "    plt.xlabel('Input Token Position')\n",
    "    plt.yticks([])\n",
    "    plt.title(f'Input Saliency for Prediction at Position {POS_TO_EXPLAIN}')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(\"> High values (brighter colors) show which *input* tokens were most\")\n",
    "    print(f\"  influential for the model's prediction at position {POS_TO_EXPLAIN}.\")\n",
    "    print(\"> This is the 'Grad-CAM' for your transformer.\")\n",
    "    print(\"> NOTE: Again, this result is random because the input is 'dummy_batch'.\")\n",
    "    print(\"  On a real Sudoku puzzle, you would see it focus on the relevant\")\n",
    "    print(\"  row, column, and 3x3 box for the cell it's trying to solve.\")\n",
    "else:\n",
    "    print(\"Saliency map calculation failed.\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS CELL COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
